{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- IMPORTS ---\n",
    "import numpy as np\n",
    "import stackstac\n",
    "import pystac_client\n",
    "import planetary_computer\n",
    "import dask.array as da\n",
    "from dask.distributed import Client, LocalCluster\n",
    "from urllib3.util.retry import Retry\n",
    "from dask.diagnostics import ProgressBar\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "import ipyleaflet\n",
    "import geopandas as gpd\n",
    "import fiona\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "spatial_resolution = 10  # Spatial resolution in meters\n",
    "bands_to_load = ['B02', 'B03', 'B04', 'SCL']  # Blue, Green, Red, Scene Classification\n",
    "local_cluster = True  # Set to False if using Coiled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- DASK CLUSTER SETUP ---\n",
    "if local_cluster:\n",
    "    cluster = LocalCluster()\n",
    "    client = Client(cluster)\n",
    "else:\n",
    "    import coiled\n",
    "    cluster = coiled.Cluster(name=\"Timelapse\", shutdown_on_close=True)\n",
    "    cluster.adapt(n_workers=1, maximum=8)\n",
    "    client = cluster.get_client()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- MAP FOR BOUNDING BOX SELECTION ---\n",
    "m = ipyleaflet.Map(scroll_wheel_zoom=True)\n",
    "m.zoom = 12\n",
    "m.layout.height = \"500px\"\n",
    "m.layout.width = \"500px\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add points from a kml file to the map, useful if making timelapses of specific locations. Provided with this script are some points in Gaza.\n",
    "\n",
    "fiona.drvsupport.supported_drivers[\"LIBKML\"] = \"rw\"\n",
    "gdf = gpd.read_file(\"points_gaza.kml\")\n",
    "\n",
    "# If the GeoDataFrame is not empty, center the map on the first point\n",
    "if not gdf.empty:\n",
    "    # Get the centroid of the first geometry (works for points and polygons)\n",
    "    first_geom = gdf.geometry.iloc[0]\n",
    "    centroid = first_geom.centroid\n",
    "    m.center = (centroid.y, centroid.x)\n",
    "\n",
    "points = ipyleaflet.GeoData(\n",
    "    geo_dataframe = gdf\n",
    ")\n",
    "m.add(points)\n",
    "display(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Wait for user to select area, then get bounding box\n",
    "bounding_box = (m.west, m.south, m.east, m.north)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the planetary computer catalogue\n",
    "catalog = pystac_client.Client.open(\n",
    "    \"https://planetarycomputer.microsoft.com/api/stac/v1\",\n",
    "    modifier=planetary_computer.sign_inplace,\n",
    ")\n",
    "\n",
    "search = catalog.search(\n",
    "    collections=[\"sentinel-2-l2a\"],\n",
    "    bbox=bounding_box,\n",
    "    datetime=\"2024-01-01/2024-12-31\"\n",
    ")\n",
    "items = search.item_collection()\n",
    "print(f\"Found {len(items)} items in the selected area and time range.\")\n",
    "\n",
    "# Stack the data using stackstac\n",
    "data = stackstac.stack(\n",
    "    items,\n",
    "    assets=bands_to_load,\n",
    "    resolution=spatial_resolution,\n",
    "    epsg=3857,\n",
    "    bounds_latlon=bounding_box,\n",
    "    chunksize = (-1, 1, 256, 256)\n",
    ")\n",
    "\n",
    "print(\"Array size information:\")\n",
    "print(f\"Shape: {data.shape}\")\n",
    "print(f\"Size in bytes: {data.data.nbytes}\")\n",
    "print(f\"Size in GB: {data.data.nbytes / 1e9:.2f} GB\")\n",
    "print(f\"Number of chunks: {data.data.npartitions}\")\n",
    "print(f\"Chunksize: {(data.data.nbytes / data.data.npartitions) / 1e6:.2f} MB\")\n",
    "\n",
    "\n",
    "# Have a look at the dataset before using it, you might want to keep the size low (sub GB) if processing on a laptop or limited network connection.\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scl = data.sel(band='SCL') # select the sentinel 2 scene classification band\n",
    "cloudy_classes = [3, 8, 9, 10, 11] # establish these pixel values as cloudy (undesirable), see sentinel 2 SCL documentation for details.\n",
    "cloud_mask = scl.isin(cloudy_classes).compute() # create a mask for all pixels that are cloudy\n",
    "clear_mask = ~cloud_mask.drop_vars('band') # invert the mask to have the opposite, all pixels that are good.\n",
    "filtered_data = data.where(clear_mask, drop = True) # filter the original dataset to remove all pixels (over all bands) where there were clouds. \n",
    "\n",
    "filled_data = filtered_data.sel(band = ['B04', 'B03', 'B02']).groupby('time.month').quantile(q = 0.5) # group the data by month, and make a composite over time being the mean of pixel values per month.\n",
    "\n",
    "filled_data\n",
    "\n",
    "# Create a new datetime index for the 15th of each month in the year of interest (placeholder, just needed for GeoGIF to display a date in the corner)\n",
    "year = 2024\n",
    "month_dates = [pd.Timestamp(year=year, month=int(m), day=15) for m in filled_data['month'].values]\n",
    "\n",
    "# Assign this as a new coordinate and swap the 'month' dimension for 'time'\n",
    "filled_data = filled_data.assign_coords(time=(\"month\", month_dates)).swap_dims({\"month\": \"time\"}).drop_vars(\"month\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Import geogif for creating animated GIFs from geospatial data\n",
    "import geogif\n",
    "\n",
    "geogif = geogif.dgif(filled_data, fps=4, date_format=None)\n",
    "geogif.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optionally - this cell can export each frame (timestamp) of the dataset as a .tif file to a new folder.\n",
    "\n",
    "import calendar\n",
    "import os\n",
    "from datetime import datetime\n",
    "import rioxarray\n",
    "\n",
    "# Get the current date and time as a string, e.g., \"2024-07-03_22-35-00\"\n",
    "current_datetime = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "output_folder = f\"monthly_composites_{current_datetime}\"\n",
    "\n",
    "# Create the output directory if it does not exist\n",
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)\n",
    "\n",
    "# Loop through each month in the 'month' dimension\n",
    "for month_num in filled_data.month.values:\n",
    "    # Select the data for the current month\n",
    "    monthly_slice = filled_data.sel(month=month_num)\n",
    "    \n",
    "    # Get the month name for a more descriptive filename\n",
    "    month_name = calendar.month_name[month_num]\n",
    "    \n",
    "    # Define the output filename inside the new folder\n",
    "    output_filename = os.path.join(output_folder, f\"monthly_composite_{month_name}.tif\")\n",
    "    \n",
    "    # Export the slice to a GeoTIFF file\n",
    "    # The 'compress' argument is optional but recommended for smaller file sizes\n",
    "    monthly_slice.rio.to_raster(output_filename, compress='LZW')\n",
    "    \n",
    "    print(f\"Exported {output_filename}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_clean_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
